{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM_Model_Training.ipynb\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from lstm_model import LSTMModel\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize LSTM model, optimizer, and criterion\n",
    "lstm_model = LSTMModel(input_size=1, hidden_layer_size=100, output_size=1)\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Move model to device\n",
    "lstm_model = lstm_model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# Load the preprocessed time series data\n",
    "preprocessed_data = pd.read_csv('data/preprocessed_time_series_data.csv')  # Update with your preprocessed dataset file\n",
    "\n",
    "# Normalize the 'value' column using Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "preprocessed_data['value_scaled'] = scaler.fit_transform(preprocessed_data[['value']])\n",
    "\n",
    "# Convert the scaled values to PyTorch tensor\n",
    "input_data = torch.tensor(preprocessed_data['value_scaled'].values, dtype=torch.float32).view(-1, 1, 1)\n",
    "\n",
    "# Convert the target values to PyTorch tensor\n",
    "target_data = torch.tensor(preprocessed_data['value_scaled'].values, dtype=torch.float32).view(-1, 1, 1)\n",
    "\n",
    "# Combine input and target sequences into DataLoader\n",
    "dataset = torch.utils.data.TensorDataset(input_data, target_data)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "def train_lstm(model, optimizer, criterion, data_loader, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for i, (input_seq, target_seq) in enumerate(data_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output_seq = model(input_seq)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(output_seq, target_seq)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Print the average loss for the epoch\n",
    "        average_loss = total_loss / len(data_loader)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "\n",
    "# Set the number of training epochs\n",
    "num_epochs = 50\n",
    "\n",
    "# Train the LSTM model\n",
    "train_lstm(lstm_model, optimizer, criterion, data_loader, num_epochs)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(lstm_model.state_dict(), 'saved_lstm_model.pt')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
